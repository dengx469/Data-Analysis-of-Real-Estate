import os
import sys
import time
import pandas as pd
from datetime import datetime
from collections import defaultdict
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from sqlalchemy import create_engine

output_dir = r"D:\tiger\realestate"
os.makedirs(output_dir, exist_ok=True)

def is_valid_data_row(row_data):
    return len(row_data) == 9 and row_data[0].endswith("åŒº")

def scrape_data_by_title_blocks(driver):
    rows = driver.find_elements(By.XPATH, "//tr")
    section_name = None
    data_blocks = defaultdict(list)

    title_map = {
        "æ¯æ—¥æ–°å»ºå•†å“æˆ¿å¯å”®ä¿¡æ¯": "å¯å”®ä¿¡æ¯",
        "æ¯æ—¥æ–°å»ºå•†å“æˆ¿æœªå”®ä¿¡æ¯": "æœªå”®ä¿¡æ¯",
        "æ¯æ—¥æ–°å»ºå•†å“æˆ¿ç­¾çº¦ä¿¡æ¯": "ç­¾çº¦ä¿¡æ¯"
    }

    for row in rows:
        text = row.text.strip()

        for k in title_map:
            if k in text:
                section_name = title_map[k]
                print(f"ğŸ” æ£€æµ‹åˆ°æ®µè½æ ‡é¢˜ï¼š{section_name}")
                break

        cols = row.find_elements(By.TAG_NAME, "td")
        row_data = [col.text.strip() for col in cols]

        if section_name and is_valid_data_row(row_data):
            data_blocks[section_name].append(row_data)
        else:
            if len(row_data) > 0 and section_name:
                print(f"âš ï¸ è·³è¿‡æ— æ•ˆè¡Œï¼ˆ{len(row_data)}åˆ—ï¼‰: {row_data}")

    return data_blocks

def scrape_housing_data():
    chrome_options = Options()
    # chrome_options.add_argument("--headless")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--window-size=1920,1080")

    service = Service(executable_path=r"D:\tiger\realestate\chromedriver-win64\chromedriver.exe")
    driver = webdriver.Chrome(service=service, options=chrome_options)

    try:
        driver.get("https://zfcj.gz.gov.cn/zfcj/tjxx/spfxstjxx")
        time.sleep(3)
        print("âœ… é¡µé¢åŠ è½½å®Œæ¯•ï¼Œå‡†å¤‡æå–æ•°æ®")

        headers = ["è¡Œæ”¿åŒº", "ä½å®…å¥—æ•°", "ä½å®…é¢ç§¯", "å•†ä¸šå¥—æ•°", "å•†ä¸šé¢ç§¯",
                   "åŠå…¬å¥—æ•°", "åŠå…¬é¢ç§¯", "è½¦ä½å¥—æ•°", "è½¦ä½é¢ç§¯"]

        raw_data = scrape_data_by_title_blocks(driver)
        final_data = {}
        for section, rows in raw_data.items():
            df = pd.DataFrame(rows, columns=headers)
            final_data[section] = df

        return final_data

    except Exception as e:
        import traceback
        traceback.print_exc()
        return None
    finally:
        driver.quit()

def save_to_excel(data_dict, output_path):
    try:
        with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
            for sheet, df in data_dict.items():
                df.to_excel(writer, sheet_name=sheet, index=False)
        print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°ï¼š{output_path}")
    except PermissionError:
        print(f"âŒ æ— æ³•ä¿å­˜ Excel æ–‡ä»¶ï¼ˆæƒé™é”™è¯¯ï¼‰ï¼Œè¯·æ£€æŸ¥æ˜¯å¦å·²æ‰“å¼€ï¼š\nâ¡ï¸ {output_path}")
        print("ğŸ”’ è¯·å…³é—­ Excel ä¸­ç›¸å…³æ–‡ä»¶åé‡è¯•ï¼Œæˆ–ä¿®æ”¹æ–‡ä»¶åã€‚")
    except Exception as e:
        print("âŒ ä¿å­˜ Excel æ–‡ä»¶æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯ï¼š", str(e))
        import traceback
        traceback.print_exc()


def save_to_mysql(data_dict, data_date):
    from sqlalchemy import text
    engine = create_engine("mysql+pymysql://root:Tiger20041130#@localhost:3306/housing?charset=utf8mb4")

    try:
        with engine.begin() as conn:  # ä½¿ç”¨äº‹åŠ¡ç®¡ç†å™¨ï¼Œè‡ªåŠ¨æäº¤æˆ–å›æ»š
            for data_type, df in data_dict.items():
                # 1ï¸âƒ£ åˆ é™¤æ—§æ•°æ®
                delete_sql = text("""
                    DELETE FROM housing_stats 
                    WHERE data_date = :data_date AND data_type = :data_type
                """)
                conn.execute(delete_sql, {"data_date": data_date, "data_type": data_type})
                print(f"ğŸ§¹ å·²æ¸…é™¤æ—§æ•°æ®ï¼š{data_date} - {data_type}")

                # 2ï¸âƒ£ å‡†å¤‡æ’å…¥æ–°æ•°æ®
                df = df.copy()
                df.insert(0, "data_type", data_type)
                df.insert(0, "data_date", data_date)
                df.columns = [
                    "data_date", "data_type", "region",
                    "residential_units", "residential_area",
                    "commercial_units", "commercial_area",
                    "office_units", "office_area",
                    "parking_units", "parking_area"
                ]

                # 3ï¸âƒ£ æ’å…¥æ–°æ•°æ®
                df.to_sql("housing_stats", con=conn, if_exists="append", index=False)
                print(f"âœ… å·²å†™å…¥æ•°æ®åº“è¡¨ housing_stats: {data_type}")

    except Exception as e:
        print("âŒ æ•°æ®åº“å†™å…¥å¤±è´¥ï¼Œå·²è‡ªåŠ¨å›æ»šï¼š", str(e))
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    if len(sys.argv) > 1:
        input_date = sys.argv[1]
        print(f"ğŸ“… ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°æ—¥æœŸï¼š{input_date}")
    else:
        input_date = input("è¯·è¾“å…¥çˆ¬å–æ—¥æœŸï¼ˆå¦‚ 20250415ï¼‰ï¼š")

    try:
        datetime.strptime(input_date, "%Y%m%d")
    except ValueError:
        print("âŒ æ—¥æœŸæ ¼å¼ä¸æ­£ç¡®ï¼Œè¯·è¾“å…¥å¦‚ 20250415")
        exit()

    filename = f"guangzhou_housing_data_{input_date}.xlsx"
    output_path = os.path.join(output_dir, filename)

    result = scrape_housing_data()
    if result:
        save_to_excel(result, output_path)
        save_to_mysql(result, datetime.strptime(input_date, "%Y%m%d").date())
        print("ğŸ“Š æŠ“å–ã€ä¿å­˜ Excel å’Œå†™å…¥æ•°æ®åº“æˆåŠŸã€‚")
        for sheet, df in result.items():
            print(f"\n[{sheet}] å‰3è¡Œç¤ºä¾‹ï¼š\n{df.head(3)}")
    else:
        print("âš ï¸ æŠ“å–å¤±è´¥ï¼Œæœªè·å–åˆ°æ•°æ®ã€‚")
